# Ollama Configuration (Optional, if using Ollama)
OLLAMA_BASE_URL=http://localhost:11434 # Your Ollama server URL if not default
OLLAMA_EMBEDDING_MODEL=nomic-embed-text # Default Ollama embedding model name
OLLAMA_GENERATION_MODEL=qwen2.5-coder:7b # Default Ollama generation model name
OLLAMA_SUMMARIZATION_MODEL=qwen2.5-coder:7b # Default Ollama summarization model name
OLLAMA_RELATIONSHIP_MODEL=qwen2.5-coder:7b # Default Ollama relationship extraction model name

# DeepSeek API Configuration (Required if using DeepSeek for any task)
# Get your API key from https://platform.deepseek.com/
DEEPSEEK_API_KEY="YOUR_DEEPSEEK_API_KEY_HERE"
DEEPSEEK_BASE_URL="https://api.deepseek.com"
# DEEPSEEK_EMBEDDING_MODEL= # No known public embedding model from DeepSeek
DEEPSEEK_CHAT_MODEL="deepseek-chat" # Example DeepSeek chat model ID

# --- Task-Specific Model Configuration ---
# Specify the model identifier for each task in the format "provider::model_name"
# Supported providers: "ollama", "deepseek"

# Model for generating embeddings (Used by RAG Handler)
# Example: Use Ollama's nomic-embed-text
EMBEDDING_MODEL_IDENTIFIER=ollama::nomic-embed-text

# Model for generating responses in RAG Q&A (Used by RAG Handler)
# Example: Use DeepSeek's chat model
# GENERATION_MODEL_IDENTIFIER=deepseek::deepseek-chat
# Example: Use Ollama's model
GENERATION_MODEL_IDENTIFIER=ollama::qwen2.5-coder:7b

# Model for summarizing code chunks (Used by Preprocessing)
# Example: Use Ollama's model
SUMMARIZATION_MODEL_IDENTIFIER=ollama::qwen2.5-coder:7b
# Example: Use DeepSeek's model (if suitable)
# SUMMARIZATION_MODEL_IDENTIFIER=deepseek::deepseek-chat

# Model for extracting relationships (Used by Preprocessing)
# Example: Use DeepSeek's model (potentially better at structured output)
# RELATIONSHIP_MODEL_IDENTIFIER=deepseek::deepseek-chat
# Example: Use Ollama's model
RELATIONSHIP_MODEL_IDENTIFIER=ollama::qwen2.5-coder:7b


# Embedding Configuration (Must match the chosen EMBEDDING_MODEL_IDENTIFIER)
# nomic-embed-text: 768
# Check model documentation for correct dimension
EMBEDDING_DIM=768

# Preprocessing Configuration
PREPROCESSING_MAX_WORKERS=4 # Number of parallel workers for LLM calls

# Data Storage Configuration
DATA_ROOT_DIR=./data
# Root directory for index and cache files

# Logging Level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Ignore Patterns File (Optional)
# Path relative to the project root, or absolute path
IGNORE_PATTERNS_FILE=./config/ignore.patterns

